{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"libraries to install for running\"\"\"\n",
    "# !pip install ipywidgets                   # Used to select different product/bank combinations to visualize\n",
    "# !pip install textacy                      # Used to quickly convert large document corpus to spacy documents\n",
    "# !pip install selenium                     # Used to scrape wallethub reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import  Options\n",
    "import time\n",
    "import datetime\n",
    "from random import uniform, choice\n",
    "\n",
    "from collections import Counter\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pathlib\n",
    "import re\n",
    "import ipywidgets as widgets\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import spacy\n",
    "import textacy\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from spacy import displacy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Sourcing - Scrapping\n",
    "Method used to scrape wallethub data for each bank\n",
    "\n",
    "**Note:** Wallethub has restrictions on the amount of hits allowed to their website which will lead to your IP being blocked if these are exceeded. Proxy IP addresses can be used to overcome this, however this is a time consuming process that took several days due to these blocks.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_proxies()-> list:\n",
    "    \"\"\"Function gets a list of free IP proxies that can be used for scraping to avoid blocks.\n",
    "\n",
    "    Returns:\n",
    "        list: returns a list of IP proxies to try\n",
    "    \"\"\"\n",
    "    url = \"https://www.us-proxy.org//\"\n",
    "    # get the HTTP response and construct soup object\n",
    "    soup = BeautifulSoup(requests.get(url).content, \"html.parser\")\n",
    "    proxies = []\n",
    "    for row in soup.find(\"table\").findAll(\"tr\"):\n",
    "        tds = row.find_all(\"td\")\n",
    "        # check allows https and is elite class\n",
    "        try:\n",
    "            # print(tds[6], tds[4])\n",
    "            if tds[6].text == 'yes' and tds[4].text == 'elite proxy':\n",
    "                ip = tds[0].text.strip()\n",
    "                port = tds[1].text.strip()\n",
    "                host = f\"{ip}:{port}\"\n",
    "                proxies.append(host)\n",
    "        except IndexError:\n",
    "            continue\n",
    "    return proxies\n",
    "\n",
    "def select_proxy(proxies:list) -> str:\n",
    "    \"\"\"Randomly choses proxy from list\n",
    "\n",
    "    Args:\n",
    "        proxies (list): list of proxies\n",
    "\n",
    "    Returns:\n",
    "        str: chosen proxy\n",
    "    \"\"\"\n",
    "    proxy = choice(proxies)\n",
    "    return proxy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_driver_session(bank:str=\"Wells Fargo\",headless:bool=True) -> object:\n",
    "    \"\"\"Creates a driver with a functioning IP proxy for scrapping.\n",
    "\n",
    "    Args:\n",
    "        bank (str, optional): Bank selected for scraping. Defaults to \"Wells Fargo\".\n",
    "        headless (bool, optional): Whether to show actions on Chrome Webbrowser. Defaults to True.\n",
    "\n",
    "    Returns:\n",
    "        object: driver session\n",
    "    \"\"\"\n",
    "    # Get Free Proxy\n",
    "    proxies = get_proxies()\n",
    "    proxy = select_proxy(proxies) # IP:PORT or HOST:PORT\n",
    "    print('Proxy IP', proxy)\n",
    "    # Set Proxy address to fetch new data\n",
    "    options = Options()\n",
    "    options.add_argument('--proxy-server=%s' % proxy)\n",
    "    # No need to see page\n",
    "    options.headless = headless\n",
    "    # Create driver\n",
    "    driver = webdriver.Chrome(DRIVER_PATH, options=options)\n",
    "    # Testing it is functioning URL\n",
    "    try:\n",
    "        driver.implicitly_wait(10)\n",
    "        driver.get(url)\n",
    "        if bank not in driver.title:\n",
    "            print(\"Did not reach review page - blocked\")\n",
    "            driver.quit()\n",
    "            create_driver_session(bank=BANK,headless=False)\n",
    "    except Exception as e:\n",
    "        print('Driver Creation Failed')\n",
    "        driver.quit()\n",
    "        create_driver_session(bank=BANK,headless=False)\n",
    "    finally:    \n",
    "        return driver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_html_file(url:str, driver:object, page:int=None) -> object:\n",
    "    \"\"\"Fetches html page of reviews.\n",
    "\n",
    "    Args:\n",
    "        url (str): path for html\n",
    "        driver (object): created drive for extraction\n",
    "        page (int, optional): page number of reviews. Defaults to None.\n",
    "\n",
    "    Returns:\n",
    "        BeautifulSoup: soup of html data used for parsing\n",
    "    \"\"\"\n",
    "    if page != None:\n",
    "        url += \"?p=\" + page\n",
    "    # inserting random time lapse to try to prevent bot detection\n",
    "    random_value = uniform(1.5,4.0)\n",
    "    print('Waited', random_value, 'seconds')\n",
    "    time.sleep(random_value) \n",
    "    # fetch website\n",
    "    driver.get(url)\n",
    "    # get html\n",
    "    soup = BeautifulSoup(driver.page_source, 'lxml', from_encoding='UTF-8')\n",
    "    return soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_page_reviews(html:object, df:pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Fetches all the page reviews from a given html and appends them to a dataframe\n",
    "\n",
    "    Args:\n",
    "        html (BeautifulSoup): Html soup used for extraction\n",
    "        df (pd.DataFrame): empty dataframe for reviews\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: filled dataframe with reviews\n",
    "    \"\"\"\n",
    "    for review in html.find_all(class_='rvtab-citem'):  \n",
    "        # Initialize Values\n",
    "        stars = 0\n",
    "        verified = True\n",
    "        review_comment = False\n",
    "        # Name & UserName\n",
    "        author_name = review.find(class_='rvtab-ci-name').text.strip()\n",
    "        nickname = review.find(class_='rvtab-ci-nickname').text[1:]\n",
    "        # Date of Review\n",
    "        date = review.find('time')['datetime']\n",
    "        # Product Reviewed\n",
    "        if review.find(class_='rvtab-ci-category') is not None:\n",
    "            product = review.find(class_='rvtab-ci-category').text.replace('Product:','').strip()\n",
    "        else:\n",
    "            # prints comments that don't belong to specific product, probably comments to previous review\n",
    "            print(author_name, nickname, date, verified, stars)\n",
    "            review_comment = True\n",
    "            product = None\n",
    "        # Review Comment\n",
    "        comment = review.find(class_='rvtab-ci-content').text\n",
    "        # Count number of stars\n",
    "        for star in review.find_all('path'):\n",
    "            if star['fill'] == '#4ae0e1':\n",
    "                stars += 1\n",
    "        if review.find(class_='rvtab-ci-verified') is None:\n",
    "            verified = False\n",
    "        # temporary to add row by row\n",
    "        row = [[date, author_name, nickname, verified, product, review_comment, stars, comment]]\n",
    "        temp = pd.DataFrame(row, columns=df.columns)\n",
    "        # add to main dataframe\n",
    "        df = df.append(temp, ignore_index=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main Function: Scrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_reviews(url:str, driver:object, start_page:int=None, data:pd.DataFrame=None) -> set[pd.DataFrame, str]:\n",
    "    \"\"\"Main function used to scrape all the data for bank webpage. If the scraping attempt fails more than 3 times the function retuns the last page and all the data scrapped up to that point\n",
    "\n",
    "    Args:\n",
    "        url (str): path for website html\n",
    "        driver (object): chromedriver used for extraction\n",
    "        start_page (int, optional): page of reviews to start from. Defaults to None.\n",
    "        data (pd.DataFrame, optional): dataframe used for storage. Defaults to None.\n",
    "\n",
    "    Returns:\n",
    "        set[pd.DataFrame, str]: dataframe with review data scrapped\n",
    "    \"\"\"\n",
    "    if data is None:\n",
    "        # Create DataFrame for storage\n",
    "        col_names = ['date', 'name', 'user_id', 'verified', 'product', 'review_comment', 'stars', 'review']\n",
    "        data = pd.DataFrame(columns=col_names)\n",
    "    # attempts counter\n",
    "    attempts = 0\n",
    "    # fetch starting page\n",
    "    html = get_html_file(url, driver)\n",
    "    # get page positions\n",
    "    page_position = html.find(class_='rvtab-pag-pos').text\n",
    "    if start_page is None:\n",
    "        current_page = page_position.split()[0]\n",
    "    else:\n",
    "        current_page = str(start_page)\n",
    "    last_page = page_position.split()[2]\n",
    "    while int(current_page) <= int(last_page):\n",
    "        print(current_page)   \n",
    "        try:\n",
    "            html = get_html_file(url, driver, current_page)\n",
    "        except Exception as e:\n",
    "            if attempts > 3:\n",
    "                break\n",
    "            print('Exception trigerred:', e)\n",
    "            driver = create_driver_session(headless=False)\n",
    "            attempts += 1\n",
    "        # check comment review exists in page - else bot may be blocked\n",
    "        if html.find_all(class_='rvtab-citem'):\n",
    "            # add page reviews\n",
    "            data = fetch_page_reviews(html, data)\n",
    "            # get new page number\n",
    "            current_page = html.find(class_='rvtab-pag-pos').text.split()[0]\n",
    "            # increase page\n",
    "            current_page = str(int(current_page) + 1)\n",
    "        elif html.title.text == 'IP Block':\n",
    "            print('No more comments retrieved - Bot Blocked')\n",
    "            if attempts > 3:\n",
    "                break\n",
    "            attempts += 1\n",
    "    driver.close()\n",
    "    return data, current_page"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dictionary of Banks & Chrome Driver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Disctionary of Bank - url post-fix\n",
    "Bank_urls = {\n",
    "    \"Wells Fargo\":\"wells-fargo-13007950i\",\n",
    "    \"Bank of America\":\"bank-of-america-13000450i\",\n",
    "    \"Chase\": \"chase-13001251i\",\n",
    "    \"Citi\": \"citibank-13001291i\",\n",
    "    \"Capital One\": \"capital-one-13001087i\",\n",
    "    \"TD Bank\": \"td-bank-13006307i\",\n",
    "    \"PNC\":\"pnc-13005045i\",\n",
    "    \"U.S. Bank\":\"us-bank-13007637i\"}\n",
    "\n",
    "\n",
    "# Path for Chomedriver\n",
    "DRIVER_PATH = \"/Users/jesidacosta/OneDrive - University of South Florida/ISM6930/group_project/chromedriver\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create DataFrame for storage\n",
    "col_names = ['date', 'name', 'user_id', 'verified', 'product', 'review_comment', 'stars', 'review']\n",
    "data = pd.DataFrame(columns=col_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Driver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start driver with Proxy IP for scrapping\n",
    "driver = create_driver_session(bank=BANK,headless=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scrapping Manually\n",
    "Run scrapping function for each bank with supervision. \n",
    "\n",
    "If the IP gets blocked up before finishing: \n",
    "1. get a new driver that is working. \n",
    "2. set starting page to the last page returned by the scrape function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select Bank to Scrape -- must match dictionary keys below\n",
    "BANK = \"Chase\"\n",
    "# Set Page number to start from -- change to 'page' if stopped midway\n",
    "PAGE = None\n",
    "# Create Url path for extraction\n",
    "url = \"https://wallethub.com/profile/\" + Bank_urls[BANK]\n",
    "\n",
    "# Run S\n",
    "data, page = scrape_reviews(url=url, driver=driver, start_page=PAGE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Data \n",
    "Saves into a data folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get date\n",
    "DATE = datetime.datetime.now().date()\n",
    "# Save Document as JSON file\n",
    "data.to_json(f'./data/{BANK}_wallethub_reviews_{DATE}.json', orient='records')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Unified Dataframe\n",
    "Goes into the data folder where all json file of reviews are stored and created a dataframe from them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Dataframe\n",
    "columns =['date', 'name', 'user_id', 'verified', 'product', 'review_comment','stars', 'review']\n",
    "df = pd.DataFrame(columns=columns)\n",
    "# Insert JSON files into unified dataframe\n",
    "for path in pathlib.Path(pathlib.os.getcwd(),'data').glob('*.json'):\n",
    "    temp = pd.read_json(path)\n",
    "    df = df.append(temp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# START FROM HERE IF NOT SCRAPPING DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read Data from CSV provided\n",
    "df = pd.read_csv('all_bank_reviews.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cleaning Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Removing non-relevant samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size before 24732\n",
      "Size after 23583\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Removing comments posted about reviews and or content posted by Site\"\"\"\n",
    "print('Size before {}'.format(df.shape[0]))\n",
    "df = df[df.name.ne('WalletHub') & df.review_comment.ne(True)].reset_index(drop=True)\n",
    "print('Size after {}'.format(df.shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assigning Bank Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>name</th>\n",
       "      <th>user_id</th>\n",
       "      <th>verified</th>\n",
       "      <th>product</th>\n",
       "      <th>review_comment</th>\n",
       "      <th>stars</th>\n",
       "      <th>review</th>\n",
       "      <th>bank</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2021-09-08</td>\n",
       "      <td>katelyn</td>\n",
       "      <td>katelyn_leifert</td>\n",
       "      <td>True</td>\n",
       "      <td>PNC Credit Cards</td>\n",
       "      <td>False</td>\n",
       "      <td>5</td>\n",
       "      <td>The PNC cash rewards card is a great first cre...</td>\n",
       "      <td>PNC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2021-09-07</td>\n",
       "      <td>Jessica K</td>\n",
       "      <td>jessicak8652</td>\n",
       "      <td>False</td>\n",
       "      <td>PNC Mortgages</td>\n",
       "      <td>False</td>\n",
       "      <td>1</td>\n",
       "      <td>Stay as far away from this lender as possible....</td>\n",
       "      <td>PNC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2021-09-06</td>\n",
       "      <td>Doris</td>\n",
       "      <td>dorish_33</td>\n",
       "      <td>True</td>\n",
       "      <td>PNC Credit Cards</td>\n",
       "      <td>False</td>\n",
       "      <td>5</td>\n",
       "      <td>I like everything about it don't want to chang...</td>\n",
       "      <td>PNC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2021-09-02</td>\n",
       "      <td>Virgil</td>\n",
       "      <td>virgilw_7</td>\n",
       "      <td>True</td>\n",
       "      <td>PNC Credit Cards</td>\n",
       "      <td>False</td>\n",
       "      <td>5</td>\n",
       "      <td>Good card....0 percent interest for first year...</td>\n",
       "      <td>PNC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2021-09-02</td>\n",
       "      <td>shloymie</td>\n",
       "      <td>shloymie</td>\n",
       "      <td>False</td>\n",
       "      <td>PNC Business Services</td>\n",
       "      <td>False</td>\n",
       "      <td>2</td>\n",
       "      <td>Local branch very Hard to reach by phone. Gene...</td>\n",
       "      <td>PNC</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        date       name          user_id verified                product  \\\n",
       "0 2021-09-08    katelyn  katelyn_leifert     True       PNC Credit Cards   \n",
       "1 2021-09-07  Jessica K     jessicak8652    False          PNC Mortgages   \n",
       "2 2021-09-06      Doris        dorish_33     True       PNC Credit Cards   \n",
       "3 2021-09-02     Virgil        virgilw_7     True       PNC Credit Cards   \n",
       "4 2021-09-02   shloymie         shloymie    False  PNC Business Services   \n",
       "\n",
       "  review_comment stars                                             review bank  \n",
       "0          False     5  The PNC cash rewards card is a great first cre...  PNC  \n",
       "1          False     1  Stay as far away from this lender as possible....  PNC  \n",
       "2          False     5  I like everything about it don't want to chang...  PNC  \n",
       "3          False     5  Good card....0 percent interest for first year...  PNC  \n",
       "4          False     2  Local branch very Hard to reach by phone. Gene...  PNC  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "banks = ['Wells Fargo', 'Bank of America', 'Citibank', 'Chase', \n",
    "         'PNC', 'TD Bank', 'Capital One', 'U.S. Bank']\n",
    "\n",
    "# If the product mentions any of the banks selected set bank label for each review\n",
    "for bank in banks:\n",
    "    df.loc[df['product'].str.contains(bank), 'bank'] = bank\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning Product Categories\n",
    "Removes the names of the banks in the product name - standardizes main products"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing bank names from product column\n",
    "pattern = re.compile(\" |\".join(banks) + \" \")\n",
    "df['product'] = df['product'].apply(lambda text: re.sub(pattern=pattern, repl=\"\", string=text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Credit Cards         17193\n",
       "Checking              3965\n",
       "Car Loans              758\n",
       "Savings & CDs          663\n",
       "Mortgages              389\n",
       "Business Services      262\n",
       "Prepaid Cards          105\n",
       "Personal Loans          87\n",
       "Home Equity             49\n",
       "Savings                 48\n",
       "Student Loans           33\n",
       "Investments             22\n",
       "CDs                      7\n",
       "CD Rates                 1\n",
       "Conventional             1\n",
       "Name: product, dtype: int64"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# See count of different product reviews\n",
    "df['product'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combining main product categories\n",
    "Combines related product categories together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before -> Rows 1 in Conventional, 17193 in Credit Cards\n",
      "After  -> Rows 0 in Conventional, 17194 in Credit Cards\n",
      "Before -> Rows 7 in CDs, 663 in Savings & CDs\n",
      "After  -> Rows 0 in CDs, 670 in Savings & CDs\n",
      "Before -> Rows 1 in CD Rates, 670 in Savings & CDs\n",
      "After  -> Rows 0 in CD Rates, 671 in Savings & CDs\n",
      "Before -> Rows 48 in Savings, 671 in Savings & CDs\n",
      "After  -> Rows 0 in Savings, 719 in Savings & CDs\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def convert_to_product(old_product:str, new_product:str) -> None:\n",
    "    \"\"\"Converts old product to a new or already exiting product category\n",
    "\n",
    "    Args:\n",
    "        old_product (str): name of old product to be converted\n",
    "        new_product (str): new or existing new to product to be assigned\n",
    "    \"\"\"\n",
    "    # affected rows\n",
    "    rows = df[df['product'].eq(old_product)].shape[0]\n",
    "    rows_new = df[df['product'].eq(new_product)].shape[0]\n",
    "    print(f'Before -> Rows {rows} in {old_product}, {rows_new} in {new_product}')\n",
    "\n",
    "    # move to new category\n",
    "    df.loc[df['product'].eq(old_product), 'product'] = new_product\n",
    "\n",
    "    # affected rows\n",
    "    rows = df[df['product'].eq(old_product)].shape[0]\n",
    "    rows_new = df[df['product'].eq(new_product)].shape[0]\n",
    "    print(f'After  -> Rows {rows} in {old_product}, {rows_new} in {new_product}')\n",
    "\n",
    "# Moving Conventional\n",
    "convert_to_product('Conventional', 'Credit Cards')\n",
    "\n",
    "# Moving CDs\n",
    "convert_to_product('CDs', 'Savings & CDs')\n",
    "\n",
    "# Moving CD Rates\n",
    "convert_to_product('CD Rates', 'Savings & CDs')\n",
    "\n",
    "# Moving Savings\n",
    "convert_to_product('Savings', 'Savings & CDs')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Removing small sample products\n",
    "\n",
    "Removes products with less than 250 reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Credit Cards         17194\n",
       "Checking              3965\n",
       "Car Loans              758\n",
       "Savings & CDs          719\n",
       "Mortgages              389\n",
       "Business Services      262\n",
       "Name: product, dtype: int64"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# REMOVE product categories with counts less than 250 --> remaining products would be 6\n",
    "df = df.groupby('product').filter(lambda x: len(x) > 250)\n",
    "\n",
    "# See new counts of remaining products\n",
    "df['product'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding year column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating year variable of comment\n",
    "df['year'] = df.date.dt.year"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sub-sampling for Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data size: 23287 rows\n"
     ]
    }
   ],
   "source": [
    "# Selecting small data slice for testing\n",
    "SUB_SAMPLE = False\n",
    "\n",
    "if SUB_SAMPLE:\n",
    "    df = df.sample(frac=.20)\n",
    "\n",
    "print('Data size: {} rows'.format(df.shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Saving Text Corpus to Textacy Corpus\n",
    "Textacy Corpus allows us to quickly parse through multiple documents\n",
    "\n",
    "Remove comment from create_corpus function execution to create a new corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# spacy library\n",
    "library = 'en_core_web_sm'\n",
    "\n",
    "# initializing nlp\n",
    "nlp = spacy.load(library)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "## function text to spacy/textacy corpus\n",
    "def create_corpus(df:pd.DataFrame, library:str='en_core_web_sm') -> textacy.Corpus:\n",
    "    \"\"\"Creates a textacy corpus with a spacy doc and meta `{'date', 'bank', 'product', 'stars'}`\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): dataframe with reviews information\n",
    "        library (str, optional): spacy pipeline used as for text processing. Defaults to 'en_core_web_sm'.\n",
    "\n",
    "    Returns:\n",
    "        textacy.Corpus: Set of spacy docs and meta\n",
    "    \"\"\"\n",
    "    # converting date to string for serialization\n",
    "    df['date'] = df['date'].astype(str)\n",
    "\n",
    "    # organize dictionary to format ['review', {meta:1, ...}]\n",
    "    # convert pandas to dictionary only of meta columns\n",
    "    columns = ['date', 'year', 'bank', 'product', 'stars']\n",
    "    meta = df[columns].to_dict(orient='records')\n",
    "    # convert reviews series to list of reviews\n",
    "    reviews = df['review'].tolist()\n",
    "    # zip into records (review, {meta})\n",
    "    records = list(zip(reviews, meta))\n",
    "    # creating base corpus\n",
    "    corpus = textacy.Corpus(lang=library)\n",
    "    # adding records (batch and n_process speeds loading)\n",
    "    corpus.add_records(records=records, batch_size=50, n_process=-1)\n",
    "    return corpus\n",
    "\n",
    "# creating corpus\n",
    "# corpus = create_corpus(df=df, library=library)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving \n",
    "# corpus.save('./data/bank_reviews.bin.gz')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corpus(23287 docs, 2495582 tokens)\n"
     ]
    }
   ],
   "source": [
    "# Loading - loading directly from bin file for faster execution \n",
    "# corpus = textacy.Corpus.load(lang=library, filepath='./data/bank_reviews.bin.gz')\n",
    "# print(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to return\n",
    "def clean_words(doc:spacy.tokens.doc.Doc, excluded:list=[], pos='None') -> list:\n",
    "    \"\"\"Cleans text into a list of words removing punctuations, stopwords, numbers and list of excluded words.\n",
    "\n",
    "    Args:\n",
    "        doc (spacy.tokens.doc.Doc): Spacy text doc\n",
    "        excluded (list, optional): list of words to exclude. Defaults to [].\n",
    "        pos (str, optional): Part of speech to select. Defaults to 'None'.\n",
    "\n",
    "    Returns:\n",
    "        list: cleaned words \n",
    "    \"\"\"\n",
    "    if pos == 'None':\n",
    "        words = [token.lemma_.lower() for token in doc if (not token.is_punct and \n",
    "                                            not token.is_stop and \n",
    "                                            token.pos_ != 'NUM' and\n",
    "                                            token.lemma_ not in excluded)]\n",
    "    else:\n",
    "        words = [token.lemma_.lower() for token in doc if (not token.is_punct and \n",
    "                                                   not token.is_stop and \n",
    "                                                   token.lemma_ not in excluded and\n",
    "                                                   token.pos_ != 'NUM' and\n",
    "                                                   token.pos_ == pos)]    # conditional for POS type\n",
    "    return words\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['bad', 'purchase', 'later', 'overdraw', 'night']"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Function to clean Documents for selected bank and product, extracting POS feature selected\n",
    "pos = None\n",
    "bank = 'Bank of America'\n",
    "product = 'Business Services'\n",
    "\n",
    "def clean_docs(product:str, bank:str='All', year='None', pos:str='None', corpus:textacy.Corpus=corpus) -> list:\n",
    "    \"\"\"Generates a filter for the bank-product selected, a list of excluded words and iteratively cleans spacy docs in the given corpus.\n",
    "\n",
    "    Args:\n",
    "        product (str): bank product to be cleaned\n",
    "        bank (str, optional): bank selected. Defaults to 'All'.\n",
    "        year (str | int): year selected\n",
    "        pos (str, optional): part of speech selected . Defaults to 'None'.\n",
    "        corpus (textacy.Corpus, optional): bank corpus to be cleaned. Defaults to corpus.\n",
    "\n",
    "    Returns:\n",
    "        list: cleaned words for all documents of given bank-product\n",
    "    \"\"\"\n",
    "    # Create empty list of words\n",
    "    words = list()\n",
    "    # filter review type by product & bank\n",
    "    if bank == 'All':\n",
    "        if year == 'None':                   \n",
    "            def filter(doc): return doc._.meta['product'] == product                                  # all banks\n",
    "        else:\n",
    "            def filter(doc): return doc._.meta['product'] == product and doc._.meta['year'] == year \n",
    "    else:\n",
    "        if year == 'None':                   \n",
    "            def filter(doc): return doc._.meta['product'] == product and doc._.meta['bank'] == bank   # show only selected bank\n",
    "        else:\n",
    "            def filter(doc): return doc._.meta['product'] == product and doc._.meta['bank'] == bank  and  doc._.meta['year'] == year\n",
    "    # Update excluded words\n",
    "    excluded = [' ', 'bank', 'account', 'tell', 'year', 'day', '$','hold', 'know', 'go', 'time', 'month']\n",
    "    # Add bank name and product name from excluded words\n",
    "    excluded += [name for name in bank.split()] + [name.rstrip('s') if name not in ('business', 'services') else name for name in product.lower().split()]\n",
    "    if bank == 'Bank of America': \n",
    "        excluded += ['boa']\n",
    "    # filtering docs\n",
    "    selected_docs = corpus.get(filter)\n",
    "    # iterate through docs\n",
    "    for doc in selected_docs:\n",
    "        top_words = clean_words(doc, excluded=excluded, pos=pos)\n",
    "        # apped to list of all docs\n",
    "        words.extend(top_words)\n",
    "    return words\n",
    "\n",
    "# example words\n",
    "clean_docs(product, bank)[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploratory Data Analysis\n",
    "\n",
    "Bar Chart, Word Cloud and Time-Varying Most Common Words requires cells be ran to display visualizations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bar Chart: Top Words/Nouns/Adjs\n",
    "\n",
    "Allows us to see the most common nouns and adjectives used for each product. We can use NOUNS as proxy for the related `aspects` and ADJECTIVES as proxy for `opinions`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1abca747acb54300aba8f5161a14ac6b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(Dropdown(description='Bank:', index=8, options=('Wells Fargo', 'Bank of America', 'Citib…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 1. create word count chart\n",
    "\n",
    "# List of unique products\n",
    "products = df['product'].unique().tolist()\n",
    "\n",
    "# Creating Dropdown menu with products\n",
    "product_dropdown = widgets.Dropdown(\n",
    "    options= [prod for prod in products],\n",
    "    value='Credit Cards',\n",
    "    description='Bank Prod:')\n",
    "\n",
    "# Creating Dropdown menu with banks\n",
    "bank_dropdown  = widgets.Dropdown(\n",
    "    options= banks + ['All'],\n",
    "    value='All',\n",
    "    description='Bank:')\n",
    "\n",
    "# Function to update bar chart\n",
    "def update_plot(bank:str, product:str, pos:str='NOUN', top_words:int=5) -> None:\n",
    "    \"\"\"Plots top 10 words or POS mentioned in the reviews for a given bank-product\n",
    "        - product: selected feature from drop down\n",
    "\n",
    "    Args:\n",
    "        bank (str): bank from drop down.\n",
    "        product (str): selected product from drop down. Defaults to 'NOUN'\n",
    "        pos (str): part of speech selected.\n",
    "        top_words (int): number of words to see\n",
    "    \"\"\"\n",
    "    # fetch clean words\n",
    "    words = clean_docs(product=product, bank=bank, pos=pos)\n",
    "    # get top words as dict\n",
    "    words_dict = {wc[0]:wc[1] for wc in Counter(words).most_common(top_words)}\n",
    "    # words_dict = reviews[product]['words']\n",
    "    plt.figure(figsize=(16,8))\n",
    "    plt.style.use(\"dark_background\")\n",
    "    sns.barplot(x=list(words_dict.keys()),\n",
    "    y=list(words_dict.values()))\n",
    "    name = pos.capitalize() if pos != 'None' else 'Word'\n",
    "    plt.title(f'Top {top_words} {name}s for {product}')\n",
    "    plt.ylabel('Counts')\n",
    "    plt.xlabel(name + 's')\n",
    "    plt.show()\n",
    "\n",
    "# Shows interactive plot to select feature and plot top 10 hotels\n",
    "widgets.interactive(update_plot, bank=bank_dropdown, product=product_dropdown, pos=[ 'NOUN', 'ADJ','None'], top_words=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Cloud for Words/Nouns/Adj\n",
    "\n",
    "Allows us to see the most common nouns and adjectives used for each product. We can use NOUNS as proxy for the related `aspects` and ADJECTIVES as proxy for `opinions`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "616945f1b5974c2c9b9f5ed38faf509f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(Dropdown(description='Bank:', index=8, options=('Wells Fargo', 'Bank of America', 'Citib…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<function __main__.make_cloud(bank: str, product: str, pos: str = 'NOUN', word_max: int = 100) -> None>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Creating Dropdown menu with products\n",
    "product_dropdown = widgets.Dropdown(\n",
    "    options= products,\n",
    "    value='Credit Cards',\n",
    "    description='Bank Prod:')\n",
    "\n",
    "# Creating Dropdown menu with banks\n",
    "bank_dropdown  = widgets.Dropdown(\n",
    "    options= banks + ['All'],\n",
    "    value='All',\n",
    "    description='Bank:')\n",
    "\n",
    "\n",
    "def make_cloud(bank:str, product:str, pos:str='NOUN',  word_max:int=100) -> None:\n",
    "    \"\"\"Creates a word cloud for given bank, product and POS with a maximun number of words selected.\n",
    "\n",
    "    Args:\n",
    "        bank (str): bank selected\n",
    "        product (str): product selected\n",
    "        pos (str, optional): part of speech seelcted. Defaults to 'NOUN'.\n",
    "        word_max (int, optional): maximun number of words to view. Defaults to 100.\n",
    "    \"\"\"\n",
    "    # get product text\n",
    "    # text = reviews[product]['text']\n",
    "    # clean text\n",
    "    cleaned_text = ' '.join(clean_docs(product, bank, pos=pos))\n",
    "    plt.figure(figsize=(16,8))\n",
    "    wordcloud = WordCloud(background_color='white', max_words=word_max).generate(cleaned_text)\n",
    "    plt.style.use(\"dark_background\")\n",
    "    name = pos.capitalize() + 's' if pos != 'None' else 'Words'\n",
    "    plt.title(f'Most used {name}: {bank}, {product}', fontsize=32)\n",
    "    plt.axis(\"off\")\n",
    "    plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
    "    \n",
    "\n",
    "widgets.interact(make_cloud, bank=bank_dropdown, product=product_dropdown, pos=['NOUN', 'ADJ', 'None'], word_max=[20,50,100,200])\n",
    "# -> Make these interactive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Time-Varying most common Noun\n",
    "Which is the most common noun per year?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "55bdf201885e4fdc98b72ce917d2e69f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(Dropdown(description='Bank:', index=2, options=('Wells Fargo', 'Bank of America', 'Citib…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<function __main__.plot_overtime(bank: str, product: str, pos: str = 'NOUN')>"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def plot_overtime(bank:str, product:str, pos:str='NOUN'):\n",
    "    # initialize dictionry for top word\n",
    "    words_per_year = dict()\n",
    "    # list of years to check\n",
    "    years = list(range(2011, 2022)) \n",
    "    for year in years:\n",
    "        words = clean_docs(product=product, bank=bank, year=year, pos=pos)\n",
    "        most_common = {year:{wc[0]:wc[1]} for wc in Counter(words).most_common(1)}\n",
    "        words_per_year.update(most_common) \n",
    "    # year\n",
    "    x = list(words_per_year.keys())\n",
    "    # list to hold count and word\n",
    "    y, word = list(), list()\n",
    "    # iterate fetching top word and count\n",
    "    for dic in words_per_year.values():\n",
    "        for k, v in dic.items():\n",
    "            y.append(v)\n",
    "            word.append(k)\n",
    "    plt.figure(figsize=(16,8))\n",
    "    # create a figure variable so each bar can be measured\n",
    "    bars = plt.bar(x, height=y, width=.4)\n",
    "    xlocs, xlabs = plt.xticks()\n",
    "    xlocs=[i for i in x]\n",
    "    xlabs=[i for i in x]\n",
    "    plt.xticks(xlocs, xlabs)\n",
    "    # plot name on top of each bar\n",
    "    for i, bar in enumerate(bars):\n",
    "        yval = bar.get_height()\n",
    "        plt.text(bar.get_x(), yval + 5, word[i])\n",
    "    # Set names & Titles\n",
    "    name = pos.capitalize() + 's' if pos != 'None' else 'Words'\n",
    "    plt.title(f'Top {name} for {product} Overtime')\n",
    "    plt.ylabel('Count')\n",
    "    plt.xlabel(name)\n",
    "    plt.show()\n",
    "    \n",
    "widgets.interact(plot_overtime, bank=bank_dropdown, product=product_dropdown, pos=['NOUN', 'ADJ', 'None'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling\n",
    "## Rule-Based Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start analyzer\n",
    "analyzer = SentimentIntensityAnalyzer()\n",
    "\n",
    "def add_compound(token:object, aspect:str) -> str:\n",
    "    \"\"\"Adds compound noun if aspect is made up of multiple nouns\n",
    "\n",
    "    Args:\n",
    "        token (object): spacy word used for parsing\n",
    "        aspect (str): already identified aspect txt to be appended to\n",
    "\n",
    "    Returns:\n",
    "        str: compound aspect if applicable\n",
    "    \"\"\"\n",
    "    for child in token.children:\n",
    "        # if child is compound then add as prefix\n",
    "        if child.dep_ == 'compound' and aspect != '99999':\n",
    "            aspect = child.norm_ + \" \" + aspect\n",
    "        # if child clause modifier add to Aspect\n",
    "        if child.dep_ == 'relcl':\n",
    "            aspect = child.norm_ + \" \" + aspect\n",
    "    return aspect\n",
    "\n",
    "def add_negative(token:object, modifier:str) ->str:\n",
    "    \"\"\"Identifies if a negative modifier is used and inserts to modifer word\n",
    "\n",
    "    Args:\n",
    "        token (object): token to be examined for negation\n",
    "        modifier (str): already identified modifier to be appended to\n",
    "\n",
    "    Returns:\n",
    "        str: negative modifier if applicable\n",
    "    \"\"\"\n",
    "    if(token.dep_ == \"neg\" and modifier != \"99999\"):\n",
    "        neg_prefix = token.norm_ if token.norm_ != \"n't\" else \"not\"\n",
    "        modifier = neg_prefix + \" \" + modifier\n",
    "    if(token.dep_ == \"det\" and token.norm_ == 'no' and modifier != \"99999\"):\n",
    "        neg_prefix = 'no'\n",
    "        modifier = neg_prefix + \" \" + modifier\n",
    "    return modifier\n",
    "\n",
    "\n",
    "def get_aspect_level_sentiments(doc:object) -> list[set]:\n",
    "    \"\"\"Main function used to extract aspect nounts and score sentiment of aspect-noun phrase.\n",
    "        \n",
    "        - Rule 1: Opinion is `amod` and Aspect is it's head\n",
    "\n",
    "        - Rule 2: Aspect is `nsubj`, Opinion is `acomp`\n",
    "\n",
    "        - Rule 3: Aspect is `nsubj`, Opinion is `attr`\n",
    "\n",
    "        - Rule 4: Aspect is `nsubj` or `nsubjpass`, Opinion is `advmod`\n",
    "\n",
    "        - Rule 5: Aspect is `dobj` && `NOUN`, Opinion is `advmod`\n",
    "    Args:\n",
    "        doc (object): review to be examined\n",
    "\n",
    "    Returns:\n",
    "        list[set]: list of review scores in sets made up of {Aspect, Modifier, SentimentScore, RuleNumber}\n",
    "    \"\"\"\n",
    "\n",
    "    ## FIRST RULE OF DEPENDANCY PARSE\n",
    "    rule_pairs = []\n",
    "\n",
    "    for token in doc:\n",
    "        A = \"99999\"               # aspect\n",
    "        O = \"99999\"               # opinion\n",
    "        if token.dep_ == \"amod\" and not token.is_stop:\n",
    "            O = token.norm_\n",
    "            A = token.head.norm_\n",
    "            if token.head.dep_ == 'dobj':\n",
    "                # print(token.head.head)\n",
    "                if token.head.head.dep_ == 'advcl':\n",
    "                    # print(token.head.head.head)\n",
    "                    O = token.head.head.head.norm_ + \" \" + token.head.head.norm_ + \" \" + O\n",
    "                else:\n",
    "                    O = token.head.head.norm_ + \" \" + O\n",
    "            for child_m in token.head.head.children:\n",
    "                O = add_negative(child_m, O)\n",
    "                for child_m2 in child_m.children:\n",
    "                    O = add_negative(child_m2, O)\n",
    "\n",
    "            # advervial modifiers (most refreshing lotion)\n",
    "            O_children = token.children\n",
    "            for child_m in O_children:\n",
    "                if (child_m.dep_ == \"advmod\"):\n",
    "                    O_hash = child_m.norm_\n",
    "                    O = O_hash + \" \" + O\n",
    "\n",
    "                O = add_negative(child_m, O)\n",
    "\n",
    "            # negation in adjective, the \"no\" keyword is a 'det' of the noun (e.g. no interesting characters)\n",
    "            A = add_compound(token.head, A)\n",
    "\n",
    "        if(A != \"99999\" and O != \"99999\"):\n",
    "            rule_pairs.append((A, O, analyzer.polarity_scores(O + \" \" + A)['compound'],1))\n",
    "\n",
    "    ## TWO RULE OF DEPENDANCY PARSE -\n",
    "    for token in doc:\n",
    "\n",
    "        children = token.children\n",
    "        A = \"99999\"\n",
    "        O = \"99999\"\n",
    "        add_neg_pfx = False\n",
    "        for child in children :\n",
    "            if(child.dep_ == \"nsubj\" and not child.is_stop):\n",
    "                A = child.norm_\n",
    "                # check_spelling(child.norm_)\n",
    "                for child_two in child.children:\n",
    "                    if child_two.dep_ == \"compound\":\n",
    "                        A = child_two.norm_ + \" \" + A\n",
    "\n",
    "            if(child.dep_ == \"acomp\" and not child.is_stop):\n",
    "                O = child.norm_\n",
    "                children_two = child.children\n",
    "                for child_two in children_two:\n",
    "                    if child_two.dep_ == 'advmod':\n",
    "                        O = child_two.norm_ + \" \" + child.norm_\n",
    "                    else:\n",
    "                        O = child.norm_\n",
    "            O = add_negative(child, O)\n",
    "\n",
    "        if(A != \"99999\" and O != \"99999\"):\n",
    "            rule_pairs.append((A, O, analyzer.polarity_scores(O + \" \" + A)['compound'],2))\n",
    "\n",
    "\n",
    "    ## THIRD RULE OF DEPENDANCY PARSE -\n",
    "    for token in doc:\n",
    "        children = token.children\n",
    "        A = \"99999\"\n",
    "        O = \"99999\"\n",
    "        add_neg_pfx = False\n",
    "        for child in children :\n",
    "            if(child.dep_ == \"nsubj\" and not child.is_stop):\n",
    "                A = child.norm_\n",
    "\n",
    "            if((child.dep_ == \"attr\") and not child.is_stop):\n",
    "                O = child.norm_\n",
    "\n",
    "            O = add_negative(child, O)\n",
    "\n",
    "        if(A != \"99999\" and O != \"99999\"):\n",
    "            rule_pairs.append((A, O,analyzer.polarity_scores(O + \" \" + A)['compound'],3))        \n",
    "\n",
    "\n",
    "    ## FOURTH RULE OF DEPENDANCY PARSE -\n",
    "    #Assumption - A verb will have only one NSUBJ and DOBJ\n",
    "    for token in doc:\n",
    "\n",
    "\n",
    "        children = token.children\n",
    "        A = \"99999\"\n",
    "        O = \"99999\"\n",
    "        add_neg_pfx = False\n",
    "        for child in children :\n",
    "            if((child.dep_ == \"nsubjpass\" or child.dep_ == \"nsubj\") and not child.is_stop):\n",
    "                A = child.norm_\n",
    "\n",
    "            if(child.dep_ == \"advmod\" and not child.is_stop):\n",
    "                O = child.norm_\n",
    "                O_children = child.children\n",
    "                for child_m in O_children:\n",
    "                    if(child_m.dep_ == \"advmod\"):\n",
    "                        O_hash = child_m.norm_\n",
    "                        O = O_hash + \" \" + child.norm_\n",
    "\n",
    "            O = add_negative(child, O)\n",
    "\n",
    "        if(A != \"99999\" and O != \"99999\"):\n",
    "            rule_pairs.append((A, O,analyzer.polarity_scores(O + \" \" + A)['compound'],4)) # )\n",
    "\n",
    "    # FIFTH RULE \n",
    "    for token in doc:\n",
    "        A = \"99999\"               \n",
    "        O = \"99999\"               \n",
    "        # Adding aspect\n",
    "        if token.dep_ == 'dobj' and token.pos_ == 'NOUN':\n",
    "            A = token.norm_\n",
    "        # Adding compound to aspect \n",
    "        A = add_compound(token=token, aspect=A)\n",
    "        # Get opinion if matching dep and pos\n",
    "        for child in token.children:\n",
    "            if child.dep_ in ('advmod') and child.pos_ in ('ADV'):\n",
    "                O = child.norm_\n",
    "            O = add_negative(child, O)\n",
    "            \n",
    "        if(A != \"99999\" and O != \"99999\"):\n",
    "            rule_pairs.append((A, O, analyzer.polarity_scores(O + \" \" + A)['compound'],5))\n",
    "    \n",
    "    # Removing pairs that do not have sentiment\n",
    "    rule_pairs = [(A,O,P,r) for (A,O,P,r) in rule_pairs if P != 0]\n",
    "\n",
    "    return rule_pairs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predicting ABSA values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitted docs: 23287\n"
     ]
    }
   ],
   "source": [
    "def predict_absa(corpus:textacy.Corpus) -> list[set]:\n",
    "    \"\"\"Makes ABSA predictions given a corpus of reviews using defined aspect_level function.\n",
    "\n",
    "    Args:\n",
    "        corpus (textacy.Corpus): corpus of reviews to make predictions\n",
    "\n",
    "    Returns:\n",
    "        list[set]: list of ABSA results in format {Aspect, Modifier, SentimentScore, RuleNumber}\n",
    "    \"\"\"\n",
    "    results = list()\n",
    "    for i, doc in enumerate(corpus):\n",
    "        aspects = get_aspect_level_sentiments(doc)\n",
    "        results.append(aspects)\n",
    "    print('Fitted docs:',len(results))\n",
    "    return results\n",
    "\n",
    "results = predict_absa(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Selecting Sample for Evaluation\n",
    "We manually used a sample of 30 reviews to extract and score.\n",
    "\n",
    "The format of the file is as follows:\n",
    "\n",
    "Index: Review\n",
    "- list of {Aspect, Opinion, SentimentScore, RuleNumber}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.random.seed(42)\n",
    "# choices = np.random.choice(range(len(corpus)),size=30)\n",
    "# choices\n",
    "\n",
    "# Seed did not work in subsequent runs so we manually selected by index for reproduction\n",
    "choices = [1603, 3226, 13381, 3189, 17654, 19797, 19639, 6077, 5168, \n",
    "           7417, 22421, 21576, 428, 501, 1201, 14108, 8679, 22716, \n",
    "           9876, 367, 20665, 16677, 15017, 167, 5339, 22228, 16510, 2789, 7403, 12701]\n",
    "\n",
    "# Saving outputs to text document for manual scoring\n",
    "with open(\"sample_evaluation.txt\", \"w\") as file:\n",
    "    file.write('Evaluation Results\\n')\n",
    "    for choice in choices:\n",
    "        file.write(f\"{choice}: {corpus[choice]} \\n\")\n",
    "        file.write(f\"\\t {get_aspect_level_sentiments(corpus[choice])} \\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Map Aspect Based Sentiments to dictionary of categories\n",
    "Mapping key aspect words captured by algorithm to match main aspects: Customer service, Interest Rate, etc.\n",
    "\n",
    "1. The aspects dictionary should contain all the hardcoded words we should identify for the main categories given.\n",
    "2. Function aggregates sentiment score per record and puts them into a dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Key words identifying main aspect categories manually extracted\n",
    "aspects = {\n",
    "    'Credit Starter':['repair', 'damaged','building'],\n",
    "    'Customer Service':['human', 'computerized', 'banker', 'refund', 'experience', 'they','live person','talk','speaking','representative','manager', 'teller', 'banker', 'lady', 'customer', 'service', 'management', 'english', 'operation', 'agent','staff'],\n",
    "    'Interest Rates': ['rate', 'interest', 'accrued'],\n",
    "    'Online Banking Services': ['transactions', 'feature','website','bills', 'tool', 'monitor', 'app', 'platform', 'online', 'menu','automatic','payment'],\n",
    "    'Rewards': ['bonus', 'offer', 'categorie','category', 'reward', 'point', 'mile', 'cash', 'back'],\n",
    "    'Fees': ['fee', 'charge', 'charges','free', 'rebate', 'deal','discount'],\n",
    "    'Security': ['fraud', 'dispute', 'security', 'seller'],\n",
    "    'Retail Branch': ['branch', 'location', 'store', 'distance', 'branches', 'atm']\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create function that will create main category scores\n",
    "def aggregate_sentiment_scores(results:list[set], aspects:dict) -> list[dict]:\n",
    "    \"\"\"Aggregates sentiment scores into specified aspect categories\n",
    "\n",
    "    Args:\n",
    "        results (list[set]): list of ABSA results\n",
    "        aspects (dict): dictionary of categories and key words matching each category\n",
    "\n",
    "    Returns:\n",
    "        list[dict]: list of dictionary with an aggregate score per review category\n",
    "    \"\"\"\n",
    "    all_scores = list()\n",
    "    # TODO: what if multiple mentions same aspect?\n",
    "\n",
    "    for result in results:\n",
    "        scores = {\n",
    "            'Credit Starter':np.nan,\n",
    "            'Customer Service': np.nan,\n",
    "            'Interest Rates': np.nan,\n",
    "            'Online Banking Services': np.nan,\n",
    "            'Rewards':np.nan,\n",
    "            'Fees':np.nan,\n",
    "            'Security':np.nan,\n",
    "            'Retail Branch': np.nan\n",
    "        }\n",
    "        # get result pair\n",
    "        for pair in result:\n",
    "            item = pair[0]\n",
    "            # for aspect word in item\n",
    "            for word in item.split():\n",
    "                # for overall aspect categories\n",
    "                for cat, words in aspects.items():\n",
    "                    # check if word in result pair is in aspect words\n",
    "                    if word.rstrip('s') in words:\n",
    "                        # add score if none existent\n",
    "                        if scores[cat] is np.nan:\n",
    "                            scores[cat] = 0\n",
    "                        scores[cat] = scores.get(cat, 0) + pair[2]\n",
    "        all_scores.append(scores)\n",
    "    return all_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardize aspect scores into main categories\n",
    "agg_scores = aggregate_sentiment_scores(results, aspects)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataframe of Aspect Values\n",
    "We create a master dataframe of records and add their corresponding selected scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_df_from_spacy(corpus:textacy.Corpus, agg_scores:list[dict]) -> pd.DataFrame:\n",
    "    \"\"\"Creates a dataframe with the matching review, product, bank and aspect scores for each review. \n",
    "    These are later used to visualize overall scores for each bank and product\n",
    "\n",
    "    Args:\n",
    "        corpus (textacy.Corpus): corpus\n",
    "        agg_scores (list[dict]): aspect scores for main categories\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: dataframe of review with accompanning score\n",
    "    \"\"\"\n",
    "    records = list()\n",
    "    for doc in corpus:\n",
    "        base = doc._.meta\n",
    "        base['text'] = doc.text\n",
    "        records.append(base)\n",
    "    # create dataframe of records\n",
    "    df = pd.DataFrame(records)\n",
    "    # creating aggregate score dataframe\n",
    "    agg_scores = pd.DataFrame(agg_scores)\n",
    "    # concatenating records and scores\n",
    "    df = pd.concat([df, agg_scores], axis=1)\n",
    "    return df\n",
    "\n",
    "# Concatenating reviews and scores\n",
    "df = create_df_from_spacy(corpus, agg_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving dataframe to CSV\n",
    "df.to_csv('final_output.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Next Steps \n",
    "Our final steps for evaluation were conducted in the following fashion:\n",
    "1. We manually scored our aspect extraction method by comparing the outputs from the algorithm versus the sample of annotated dataset\n",
    "2. We used PowerBI to visualize the overall aspect category scores and draw insights and recommendations from our analysis"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "03bfd060fb8b62b12cb6c2c7a2e99139baaf1a28976ffe5231208b56dd8dc23e"
  },
  "kernelspec": {
   "display_name": "Python 3.9.4 64-bit ('ISM6930-CY7GBNHA': pipenv)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
